
==> Audit <==
|------------|-------------------|----------|---------|---------|---------------------|---------------------|
|  Command   |       Args        | Profile  |  User   | Version |     Start Time      |      End Time       |
|------------|-------------------|----------|---------|---------|---------------------|---------------------|
| stop       |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:05 +07 | 17 Sep 24 14:05 +07 |
| start      |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:05 +07 | 17 Sep 24 14:06 +07 |
| ip         |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:08 +07 | 17 Sep 24 14:08 +07 |
| tunnel     |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:13 +07 | 17 Sep 24 14:16 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:17 +07 | 17 Sep 24 14:17 +07 |
| service    | service-b --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:17 +07 | 17 Sep 24 14:18 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:18 +07 | 17 Sep 24 14:19 +07 |
| docker-env |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:19 +07 | 17 Sep 24 14:19 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:20 +07 | 17 Sep 24 14:22 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:22 +07 | 17 Sep 24 14:24 +07 |
| service    | service-b --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:24 +07 | 17 Sep 24 14:27 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:37 +07 | 17 Sep 24 14:38 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:40 +07 | 17 Sep 24 15:38 +07 |
| docker-env |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:42 +07 | 17 Sep 24 14:42 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 14:43 +07 | 17 Sep 24 14:44 +07 |
| ip         |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 15:10 +07 | 17 Sep 24 15:10 +07 |
| ip         |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 15:22 +07 | 17 Sep 24 15:22 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 15:39 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 15:56 +07 |                     |
| service    | service-b --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 15:59 +07 |                     |
| docker-env |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:03 +07 | 17 Sep 24 16:03 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:03 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:04 +07 |                     |
| stop       |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:09 +07 | 17 Sep 24 16:09 +07 |
| start      |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:09 +07 | 17 Sep 24 16:10 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:10 +07 |                     |
| tunnel     | --cleanup         | minikube | a677235 | v1.34.0 | 17 Sep 24 16:11 +07 | 17 Sep 24 16:11 +07 |
| tunnel     |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:11 +07 |                     |
| tunnel     | --cleanup         | minikube | a677235 | v1.34.0 | 17 Sep 24 16:11 +07 | 17 Sep 24 16:11 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:12 +07 |                     |
| delete     |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:12 +07 | 17 Sep 24 16:12 +07 |
| start      |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:12 +07 | 17 Sep 24 16:12 +07 |
| docker-env |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:15 +07 | 17 Sep 24 16:15 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:29 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:32 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:33 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:36 +07 |                     |
| ip         |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:37 +07 | 17 Sep 24 16:37 +07 |
| tunnel     |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:39 +07 | 17 Sep 24 16:42 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:39 +07 |                     |
| tunnel     | --cleanup         | minikube | a677235 | v1.34.0 | 17 Sep 24 16:42 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:42 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:50 +07 |                     |
| tunnel     |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:52 +07 | 17 Sep 24 16:52 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:52 +07 |                     |
| stop       |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:53 +07 | 17 Sep 24 16:54 +07 |
| start      |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:54 +07 | 17 Sep 24 16:54 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:54 +07 |                     |
| tunnel     |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:58 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 16:58 +07 |                     |
| delete     |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:00 +07 | 17 Sep 24 17:00 +07 |
| start      | --driver=hyperkit | minikube | a677235 | v1.34.0 | 17 Sep 24 17:00 +07 |                     |
| start      | --driver=hyperkit | minikube | a677235 | v1.34.0 | 17 Sep 24 17:01 +07 |                     |
| start      |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:01 +07 | 17 Sep 24 17:01 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:02 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:04 +07 |                     |
| docker-env |                   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:05 +07 | 17 Sep 24 17:05 +07 |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:16 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:18 +07 |                     |
| service    | service-a --url   | minikube | a677235 | v1.34.0 | 17 Sep 24 17:19 +07 |                     |
|------------|-------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/09/17 17:01:12
Running on machine: AR8040677235N1
Binary: Built with gc go1.22.5 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0917 17:01:12.783398   58530 out.go:345] Setting OutFile to fd 1 ...
I0917 17:01:12.783732   58530 out.go:397] isatty.IsTerminal(1) = true
I0917 17:01:12.783734   58530 out.go:358] Setting ErrFile to fd 2...
I0917 17:01:12.783736   58530 out.go:397] isatty.IsTerminal(2) = true
I0917 17:01:12.783826   58530 root.go:338] Updating PATH: /Users/a677235/.minikube/bin
I0917 17:01:12.786852   58530 out.go:352] Setting JSON to false
I0917 17:01:13.035735   58530 start.go:129] hostinfo: {"hostname":"AR8040677235N1.local","uptime":1915171,"bootTime":1724652101,"procs":698,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.5","kernelVersion":"23.5.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"fac77352-85ea-591c-b13f-beb75a699c06"}
W0917 17:01:13.035851   58530 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0917 17:01:13.040514   58530 out.go:177] üòÑ  minikube v1.34.0 on Darwin 14.5 (arm64)
I0917 17:01:13.049416   58530 notify.go:220] Checking for updates...
I0917 17:01:13.057055   58530 driver.go:394] Setting default libvirt URI to qemu:///system
I0917 17:01:13.057080   58530 global.go:112] Querying for installed drivers using PATH=/Users/a677235/.minikube/bin:/opt/homebrew/opt/go@1.21/bin:/Users/a677235/Downloads/google-cloud-sdk/bin:/Users/a677235/go/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/usr/local/sbin:/usr/local/go/bin:/opt/homebrew/opt/go@1.21/bin:/Users/a677235/Downloads/google-cloud-sdk/bin:/Users/a677235/go/bin
I0917 17:01:13.234412   58530 docker.go:123] docker version: linux-26.1.4:Docker Desktop 4.31.0 (153195)
I0917 17:01:13.234542   58530 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0917 17:01:13.592283   58530 info.go:266] docker info: {ID:165b7669-7370-4e66-85ff-767809c6681f Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:32 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:false NGoroutines:82 SystemTime:2024-09-17 10:01:13.578670094 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.31-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8221421568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/a677235/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/a677235/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/Users/a677235/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/Users/a677235/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/Users/a677235/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/a677235/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/Users/a677235/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/a677235/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/Users/a677235/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/a677235/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0917 17:01:13.592364   58530 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
W0917 17:01:14.050646   58530 podman.go:138] podman returned error: exit status 125
I0917 17:01:14.050697   58530 global.go:133] podman default: true priority: 3, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:"podman version --format {{.Server.Version}}" exit status 125: Cannot connect to Podman. Please verify your connection to the Linux system using `podman system connection list`, or try `podman machine init` and `podman machine start` to manage a new Linux VM
Error: unable to connect to Podman socket: failed to connect: dial tcp 127.0.0.1:55729: connect: connection refused Reason: Fix: Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0917 17:01:14.050812   58530 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0917 17:01:14.050864   58530 global.go:133] vfkit default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vfkit": executable file not found in $PATH Reason: Fix:Run 'brew tap cfergeau/crc && brew install vfkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vfkit/ Version:}
I0917 17:01:14.050918   58530 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0917 17:01:14.050961   58530 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0917 17:01:14.050965   58530 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0917 17:01:14.050996   58530 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0917 17:01:14.051033   58530 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-aarch64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0917 17:01:14.051047   58530 driver.go:316] not recommending "ssh" due to default: false
I0917 17:01:14.051049   58530 driver.go:311] not recommending "podman" due to health: "podman version --format {{.Server.Version}}" exit status 125: Cannot connect to Podman. Please verify your connection to the Linux system using `podman system connection list`, or try `podman machine init` and `podman machine start` to manage a new Linux VM
Error: unable to connect to Podman socket: failed to connect: dial tcp 127.0.0.1:55729: connect: connection refused
I0917 17:01:14.051060   58530 driver.go:351] Picked: docker
I0917 17:01:14.051063   58530 driver.go:352] Alternatives: [ssh]
I0917 17:01:14.051065   58530 driver.go:353] Rejects: [podman hyperkit vfkit virtualbox vmware parallels qemu2]
I0917 17:01:14.059202   58530 out.go:177] ‚ú®  Automatically selected the docker driver
I0917 17:01:14.063071   58530 start.go:297] selected driver: docker
I0917 17:01:14.063077   58530 start.go:901] validating driver "docker" against <nil>
I0917 17:01:14.063084   58530 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0917 17:01:14.063177   58530 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0917 17:01:14.243212   58530 info.go:266] docker info: {ID:165b7669-7370-4e66-85ff-767809c6681f Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:32 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:false NGoroutines:82 SystemTime:2024-09-17 10:01:14.223529469 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.31-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8221421568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/a677235/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/a677235/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/Users/a677235/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/Users/a677235/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/Users/a677235/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/a677235/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/Users/a677235/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/a677235/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/Users/a677235/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/a677235/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0917 17:01:14.243351   58530 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0917 17:01:14.422486   58530 start_flags.go:393] Using suggested 7792MB memory alloc based on sys=36864MB, container=7840MB
I0917 17:01:14.422602   58530 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0917 17:01:14.426808   58530 out.go:177] üìå  Using Docker Desktop driver with root privileges
I0917 17:01:14.430641   58530 cni.go:84] Creating CNI manager for ""
I0917 17:01:14.430654   58530 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 17:01:14.430673   58530 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0917 17:01:14.430723   58530 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7792 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0917 17:01:14.438760   58530 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0917 17:01:14.450066   58530 cache.go:121] Beginning downloading kic base image for docker with docker
I0917 17:01:14.457030   58530 out.go:177] üöú  Pulling base image v0.0.45 ...
I0917 17:01:14.464753   58530 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0917 17:01:14.464769   58530 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0917 17:01:14.464798   58530 preload.go:146] Found local preload: /Users/a677235/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4
I0917 17:01:14.464802   58530 cache.go:56] Caching tarball of preloaded images
I0917 17:01:14.467761   58530 preload.go:172] Found /Users/a677235/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0917 17:01:14.468096   58530 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0917 17:01:14.470627   58530 profile.go:143] Saving config to /Users/a677235/.minikube/profiles/minikube/config.json ...
I0917 17:01:14.470934   58530 lock.go:35] WriteFile acquiring /Users/a677235/.minikube/profiles/minikube/config.json: {Name:mk94a3dde776034372974b67555bc9cafce64fba Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
W0917 17:01:14.678877   58530 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I0917 17:01:14.678886   58530 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0917 17:01:14.679840   58530 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0917 17:01:14.679864   58530 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0917 17:01:14.679870   58530 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0917 17:01:14.679889   58530 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0917 17:01:14.679891   58530 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0917 17:01:15.269685   58530 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I0917 17:01:15.269747   58530 cache.go:194] Successfully downloaded all kic artifacts
I0917 17:01:15.269779   58530 start.go:360] acquireMachinesLock for minikube: {Name:mka5df14599c8fb57818dc5a7b1b1d789ca40b7a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0917 17:01:15.270913   58530 start.go:364] duration metric: took 1.12275ms to acquireMachinesLock for "minikube"
I0917 17:01:15.270951   58530 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7792 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0917 17:01:15.271022   58530 start.go:125] createHost starting for "" (driver="docker")
I0917 17:01:15.279309   58530 out.go:235] üî•  Creating docker container (CPUs=2, Memory=7792MB) ...
I0917 17:01:15.279554   58530 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0917 17:01:15.279572   58530 client.go:168] LocalClient.Create starting
I0917 17:01:15.280718   58530 main.go:141] libmachine: Reading certificate data from /Users/a677235/.minikube/certs/ca.pem
I0917 17:01:15.281422   58530 main.go:141] libmachine: Decoding PEM data...
I0917 17:01:15.281433   58530 main.go:141] libmachine: Parsing certificate...
I0917 17:01:15.281514   58530 main.go:141] libmachine: Reading certificate data from /Users/a677235/.minikube/certs/cert.pem
I0917 17:01:15.282139   58530 main.go:141] libmachine: Decoding PEM data...
I0917 17:01:15.282149   58530 main.go:141] libmachine: Parsing certificate...
I0917 17:01:15.282768   58530 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0917 17:01:15.445036   58530 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0917 17:01:15.445111   58530 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0917 17:01:15.445118   58530 cli_runner.go:164] Run: docker network inspect minikube
W0917 17:01:15.718141   58530 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0917 17:01:15.718172   58530 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0917 17:01:15.718179   58530 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0917 17:01:15.718340   58530 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0917 17:01:15.940847   58530 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x1400157e5b0}
I0917 17:01:15.940887   58530 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I0917 17:01:15.940984   58530 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0917 17:01:16.130123   58530 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0917 17:01:16.130156   58530 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0917 17:01:16.130358   58530 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0917 17:01:16.323974   58530 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0917 17:01:16.534861   58530 oci.go:103] Successfully created a docker volume minikube
I0917 17:01:16.535057   58530 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib
I0917 17:01:17.689930   58530 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -d /var/lib: (1.154839083s)
I0917 17:01:17.689964   58530 oci.go:107] Successfully prepared a docker volume minikube
I0917 17:01:17.690007   58530 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0917 17:01:17.690029   58530 kic.go:194] Starting extracting preloaded images to volume ...
I0917 17:01:17.690187   58530 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/a677235/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir
I0917 17:01:20.909224   58530 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/a677235/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 -I lz4 -xf /preloaded.tar -C /extractDir: (3.218999792s)
I0917 17:01:20.909332   58530 kic.go:203] duration metric: took 3.219331167s to extract preloaded images to volume ...
I0917 17:01:20.909538   58530 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0917 17:01:21.519684   58530 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7792mb --memory-swap=7792mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85
I0917 17:01:21.911718   58530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0917 17:01:22.142732   58530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 17:01:22.283627   58530 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0917 17:01:22.426978   58530 oci.go:144] the created container "minikube" has a running status.
I0917 17:01:22.427021   58530 kic.go:225] Creating ssh key for kic: /Users/a677235/.minikube/machines/minikube/id_rsa...
I0917 17:01:22.540310   58530 kic_runner.go:191] docker (temp): /Users/a677235/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0917 17:01:22.760951   58530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 17:01:22.895228   58530 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0917 17:01:22.895240   58530 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0917 17:01:23.050927   58530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 17:01:23.159354   58530 machine.go:93] provisionDockerMachine start ...
I0917 17:01:23.159526   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:23.277271   58530 main.go:141] libmachine: Using SSH client type: native
I0917 17:01:23.278367   58530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009505a0] 0x100952e00 <nil>  [] 0s} 127.0.0.1 58048 <nil> <nil>}
I0917 17:01:23.278373   58530 main.go:141] libmachine: About to run SSH command:
hostname
I0917 17:01:23.396412   58530 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0917 17:01:23.396438   58530 ubuntu.go:169] provisioning hostname "minikube"
I0917 17:01:23.396554   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:23.517381   58530 main.go:141] libmachine: Using SSH client type: native
I0917 17:01:23.518037   58530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009505a0] 0x100952e00 <nil>  [] 0s} 127.0.0.1 58048 <nil> <nil>}
I0917 17:01:23.518044   58530 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0917 17:01:23.646225   58530 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0917 17:01:23.646376   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:23.769125   58530 main.go:141] libmachine: Using SSH client type: native
I0917 17:01:23.769812   58530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009505a0] 0x100952e00 <nil>  [] 0s} 127.0.0.1 58048 <nil> <nil>}
I0917 17:01:23.769828   58530 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0917 17:01:23.887708   58530 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0917 17:01:23.887726   58530 ubuntu.go:175] set auth options {CertDir:/Users/a677235/.minikube CaCertPath:/Users/a677235/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/a677235/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/a677235/.minikube/machines/server.pem ServerKeyPath:/Users/a677235/.minikube/machines/server-key.pem ClientKeyPath:/Users/a677235/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/a677235/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/a677235/.minikube}
I0917 17:01:23.887746   58530 ubuntu.go:177] setting up certificates
I0917 17:01:23.887752   58530 provision.go:84] configureAuth start
I0917 17:01:23.887841   58530 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 17:01:24.029648   58530 provision.go:143] copyHostCerts
I0917 17:01:24.029907   58530 exec_runner.go:144] found /Users/a677235/.minikube/ca.pem, removing ...
I0917 17:01:24.029911   58530 exec_runner.go:203] rm: /Users/a677235/.minikube/ca.pem
I0917 17:01:24.030309   58530 exec_runner.go:151] cp: /Users/a677235/.minikube/certs/ca.pem --> /Users/a677235/.minikube/ca.pem (1078 bytes)
I0917 17:01:24.030932   58530 exec_runner.go:144] found /Users/a677235/.minikube/cert.pem, removing ...
I0917 17:01:24.030936   58530 exec_runner.go:203] rm: /Users/a677235/.minikube/cert.pem
I0917 17:01:24.031041   58530 exec_runner.go:151] cp: /Users/a677235/.minikube/certs/cert.pem --> /Users/a677235/.minikube/cert.pem (1123 bytes)
I0917 17:01:24.031588   58530 exec_runner.go:144] found /Users/a677235/.minikube/key.pem, removing ...
I0917 17:01:24.031590   58530 exec_runner.go:203] rm: /Users/a677235/.minikube/key.pem
I0917 17:01:24.031682   58530 exec_runner.go:151] cp: /Users/a677235/.minikube/certs/key.pem --> /Users/a677235/.minikube/key.pem (1675 bytes)
I0917 17:01:24.032052   58530 provision.go:117] generating server cert: /Users/a677235/.minikube/machines/server.pem ca-key=/Users/a677235/.minikube/certs/ca.pem private-key=/Users/a677235/.minikube/certs/ca-key.pem org=a677235.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0917 17:01:24.091948   58530 provision.go:177] copyRemoteCerts
I0917 17:01:24.092181   58530 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0917 17:01:24.092215   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:24.202338   58530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58048 SSHKeyPath:/Users/a677235/.minikube/machines/minikube/id_rsa Username:docker}
I0917 17:01:24.298766   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0917 17:01:24.317358   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0917 17:01:24.331424   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0917 17:01:24.345781   58530 provision.go:87] duration metric: took 458.018208ms to configureAuth
I0917 17:01:24.345794   58530 ubuntu.go:193] setting minikube options for container-runtime
I0917 17:01:24.346918   58530 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0917 17:01:24.346993   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:24.451988   58530 main.go:141] libmachine: Using SSH client type: native
I0917 17:01:24.452514   58530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009505a0] 0x100952e00 <nil>  [] 0s} 127.0.0.1 58048 <nil> <nil>}
I0917 17:01:24.452519   58530 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0917 17:01:24.567095   58530 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0917 17:01:24.567105   58530 ubuntu.go:71] root file system type: overlay
I0917 17:01:24.567178   58530 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0917 17:01:24.567264   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:24.700725   58530 main.go:141] libmachine: Using SSH client type: native
I0917 17:01:24.701196   58530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009505a0] 0x100952e00 <nil>  [] 0s} 127.0.0.1 58048 <nil> <nil>}
I0917 17:01:24.701256   58530 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0917 17:01:24.823881   58530 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0917 17:01:24.824016   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:24.951815   58530 main.go:141] libmachine: Using SSH client type: native
I0917 17:01:24.952535   58530 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1009505a0] 0x100952e00 <nil>  [] 0s} 127.0.0.1 58048 <nil> <nil>}
I0917 17:01:24.952550   58530 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0917 17:01:25.331550   58530 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-09-17 10:01:24.821248001 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0917 17:01:25.331571   58530 machine.go:96] duration metric: took 2.1722195s to provisionDockerMachine
I0917 17:01:25.331578   58530 client.go:171] duration metric: took 10.052101583s to LocalClient.Create
I0917 17:01:25.331604   58530 start.go:167] duration metric: took 10.052148084s to libmachine.API.Create "minikube"
I0917 17:01:25.331610   58530 start.go:293] postStartSetup for "minikube" (driver="docker")
I0917 17:01:25.331617   58530 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0917 17:01:25.331766   58530 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0917 17:01:25.331828   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:25.473129   58530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58048 SSHKeyPath:/Users/a677235/.minikube/machines/minikube/id_rsa Username:docker}
I0917 17:01:25.555764   58530 ssh_runner.go:195] Run: cat /etc/os-release
I0917 17:01:25.558104   58530 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0917 17:01:25.558120   58530 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0917 17:01:25.558130   58530 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0917 17:01:25.558134   58530 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0917 17:01:25.558140   58530 filesync.go:126] Scanning /Users/a677235/.minikube/addons for local assets ...
I0917 17:01:25.558435   58530 filesync.go:126] Scanning /Users/a677235/.minikube/files for local assets ...
I0917 17:01:25.558582   58530 start.go:296] duration metric: took 226.970333ms for postStartSetup
I0917 17:01:25.560027   58530 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 17:01:25.688406   58530 profile.go:143] Saving config to /Users/a677235/.minikube/profiles/minikube/config.json ...
I0917 17:01:25.689757   58530 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0917 17:01:25.689814   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:25.809536   58530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58048 SSHKeyPath:/Users/a677235/.minikube/machines/minikube/id_rsa Username:docker}
I0917 17:01:25.892805   58530 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0917 17:01:25.896337   58530 start.go:128] duration metric: took 10.625403291s to createHost
I0917 17:01:25.896350   58530 start.go:83] releasing machines lock for "minikube", held for 10.625534s
I0917 17:01:25.896449   58530 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0917 17:01:26.058859   58530 ssh_runner.go:195] Run: cat /version.json
I0917 17:01:26.058923   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:26.059664   58530 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0917 17:01:26.059788   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:26.267737   58530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58048 SSHKeyPath:/Users/a677235/.minikube/machines/minikube/id_rsa Username:docker}
I0917 17:01:26.268040   58530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58048 SSHKeyPath:/Users/a677235/.minikube/machines/minikube/id_rsa Username:docker}
I0917 17:01:26.576363   58530 ssh_runner.go:195] Run: systemctl --version
I0917 17:01:26.579803   58530 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0917 17:01:26.583731   58530 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0917 17:01:26.599026   58530 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0917 17:01:26.599103   58530 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0917 17:01:26.611494   58530 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0917 17:01:26.611504   58530 start.go:495] detecting cgroup driver to use...
I0917 17:01:26.611513   58530 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0917 17:01:26.611606   58530 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0917 17:01:26.618301   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0917 17:01:26.622709   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0917 17:01:26.626733   58530 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0917 17:01:26.626790   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0917 17:01:26.631271   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0917 17:01:26.636381   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0917 17:01:26.640835   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0917 17:01:26.645240   58530 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0917 17:01:26.648943   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0917 17:01:26.653567   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0917 17:01:26.658467   58530 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0917 17:01:26.663587   58530 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0917 17:01:26.667639   58530 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0917 17:01:26.672255   58530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 17:01:26.705526   58530 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0917 17:01:26.758735   58530 start.go:495] detecting cgroup driver to use...
I0917 17:01:26.758753   58530 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0917 17:01:26.758876   58530 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0917 17:01:26.771838   58530 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0917 17:01:26.771947   58530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0917 17:01:26.778345   58530 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0917 17:01:26.786117   58530 ssh_runner.go:195] Run: which cri-dockerd
I0917 17:01:26.788566   58530 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0917 17:01:26.793388   58530 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0917 17:01:26.801966   58530 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0917 17:01:26.839364   58530 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0917 17:01:26.869281   58530 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0917 17:01:26.869428   58530 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0917 17:01:26.877467   58530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 17:01:26.908256   58530 ssh_runner.go:195] Run: sudo systemctl restart docker
I0917 17:01:27.021285   58530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0917 17:01:27.026987   58530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0917 17:01:27.032656   58530 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0917 17:01:27.064840   58530 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0917 17:01:27.099940   58530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 17:01:27.132350   58530 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0917 17:01:27.147916   58530 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0917 17:01:27.153595   58530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 17:01:27.185249   58530 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0917 17:01:27.240017   58530 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0917 17:01:27.240379   58530 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0917 17:01:27.242727   58530 start.go:563] Will wait 60s for crictl version
I0917 17:01:27.242803   58530 ssh_runner.go:195] Run: which crictl
I0917 17:01:27.245228   58530 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0917 17:01:27.267708   58530 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0917 17:01:27.267793   58530 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0917 17:01:27.283670   58530 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0917 17:01:27.300023   58530 out.go:235] üê≥  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0917 17:01:27.300185   58530 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0917 17:01:27.490974   58530 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0917 17:01:27.491412   58530 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0917 17:01:27.494234   58530 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0917 17:01:27.500943   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0917 17:01:27.621240   58530 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7792 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0917 17:01:27.621349   58530 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0917 17:01:27.621436   58530 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0917 17:01:27.636107   58530 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0917 17:01:27.636116   58530 docker.go:615] Images already preloaded, skipping extraction
I0917 17:01:27.636227   58530 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0917 17:01:27.646994   58530 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0917 17:01:27.647007   58530 cache_images.go:84] Images are preloaded, skipping loading
I0917 17:01:27.647014   58530 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0917 17:01:27.647137   58530 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0917 17:01:27.647228   58530 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0917 17:01:27.671723   58530 cni.go:84] Creating CNI manager for ""
I0917 17:01:27.671739   58530 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 17:01:27.671752   58530 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0917 17:01:27.671783   58530 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0917 17:01:27.671909   58530 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0917 17:01:27.672022   58530 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0917 17:01:27.676380   58530 binaries.go:44] Found k8s binaries, skipping transfer
I0917 17:01:27.676481   58530 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0917 17:01:27.680622   58530 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0917 17:01:27.688896   58530 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0917 17:01:27.698178   58530 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0917 17:01:27.707225   58530 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0917 17:01:27.709235   58530 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0917 17:01:27.715307   58530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 17:01:27.746505   58530 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0917 17:01:27.762387   58530 certs.go:68] Setting up /Users/a677235/.minikube/profiles/minikube for IP: 192.168.49.2
I0917 17:01:27.762397   58530 certs.go:194] generating shared ca certs ...
I0917 17:01:27.762409   58530 certs.go:226] acquiring lock for ca certs: {Name:mk0d602f8d47f54dd79b245096e3c609e4fe7c52 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:27.765218   58530 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/a677235/.minikube/ca.key
I0917 17:01:27.766153   58530 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/a677235/.minikube/proxy-client-ca.key
I0917 17:01:27.766181   58530 certs.go:256] generating profile certs ...
I0917 17:01:27.766257   58530 certs.go:363] generating signed profile cert for "minikube-user": /Users/a677235/.minikube/profiles/minikube/client.key
I0917 17:01:27.766269   58530 crypto.go:68] Generating cert /Users/a677235/.minikube/profiles/minikube/client.crt with IP's: []
I0917 17:01:27.815890   58530 crypto.go:156] Writing cert to /Users/a677235/.minikube/profiles/minikube/client.crt ...
I0917 17:01:27.815898   58530 lock.go:35] WriteFile acquiring /Users/a677235/.minikube/profiles/minikube/client.crt: {Name:mkd56e37f6401e0c8fa21079a11687e049fea83c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:27.817201   58530 crypto.go:164] Writing key to /Users/a677235/.minikube/profiles/minikube/client.key ...
I0917 17:01:27.817205   58530 lock.go:35] WriteFile acquiring /Users/a677235/.minikube/profiles/minikube/client.key: {Name:mka988c2c37be4ea846440090bdecde6a402e311 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:27.818164   58530 certs.go:363] generating signed profile cert for "minikube": /Users/a677235/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0917 17:01:27.818173   58530 crypto.go:68] Generating cert /Users/a677235/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0917 17:01:27.846931   58530 crypto.go:156] Writing cert to /Users/a677235/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0917 17:01:27.846946   58530 lock.go:35] WriteFile acquiring /Users/a677235/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk76cc6711099cc1c1a64056832cd0e77ff63d59 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:27.848282   58530 crypto.go:164] Writing key to /Users/a677235/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0917 17:01:27.848290   58530 lock.go:35] WriteFile acquiring /Users/a677235/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk0580d65f8f5ed36aa6318e34447dd899f2c18c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:27.849342   58530 certs.go:381] copying /Users/a677235/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/a677235/.minikube/profiles/minikube/apiserver.crt
I0917 17:01:27.850292   58530 certs.go:385] copying /Users/a677235/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/a677235/.minikube/profiles/minikube/apiserver.key
I0917 17:01:27.851330   58530 certs.go:363] generating signed profile cert for "aggregator": /Users/a677235/.minikube/profiles/minikube/proxy-client.key
I0917 17:01:27.851339   58530 crypto.go:68] Generating cert /Users/a677235/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0917 17:01:27.920089   58530 crypto.go:156] Writing cert to /Users/a677235/.minikube/profiles/minikube/proxy-client.crt ...
I0917 17:01:27.920101   58530 lock.go:35] WriteFile acquiring /Users/a677235/.minikube/profiles/minikube/proxy-client.crt: {Name:mk71e7a5af35a231fa8e56778bdbde9a0ee78bab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:27.921310   58530 crypto.go:164] Writing key to /Users/a677235/.minikube/profiles/minikube/proxy-client.key ...
I0917 17:01:27.921316   58530 lock.go:35] WriteFile acquiring /Users/a677235/.minikube/profiles/minikube/proxy-client.key: {Name:mkbb146fb893096a9af51af51e4183412505372a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:27.925503   58530 certs.go:484] found cert: /Users/a677235/.minikube/certs/ca-key.pem (1679 bytes)
I0917 17:01:27.925701   58530 certs.go:484] found cert: /Users/a677235/.minikube/certs/ca.pem (1078 bytes)
I0917 17:01:27.925876   58530 certs.go:484] found cert: /Users/a677235/.minikube/certs/cert.pem (1123 bytes)
I0917 17:01:27.926042   58530 certs.go:484] found cert: /Users/a677235/.minikube/certs/key.pem (1675 bytes)
I0917 17:01:27.926715   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0917 17:01:27.938650   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0917 17:01:27.951903   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0917 17:01:27.966980   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0917 17:01:27.979098   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0917 17:01:27.991594   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0917 17:01:28.005467   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0917 17:01:28.017868   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0917 17:01:28.029578   58530 ssh_runner.go:362] scp /Users/a677235/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0917 17:01:28.045373   58530 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0917 17:01:28.058423   58530 ssh_runner.go:195] Run: openssl version
I0917 17:01:28.061528   58530 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0917 17:01:28.068481   58530 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0917 17:01:28.070898   58530 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 16 04:29 /usr/share/ca-certificates/minikubeCA.pem
I0917 17:01:28.070941   58530 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0917 17:01:28.074989   58530 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0917 17:01:28.079389   58530 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0917 17:01:28.081319   58530 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0917 17:01:28.081418   58530 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:7792 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0917 17:01:28.081548   58530 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0917 17:01:28.092404   58530 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0917 17:01:28.097910   58530 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0917 17:01:28.104475   58530 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0917 17:01:28.104558   58530 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0917 17:01:28.110289   58530 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0917 17:01:28.110297   58530 kubeadm.go:157] found existing configuration files:

I0917 17:01:28.110367   58530 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0917 17:01:28.115594   58530 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0917 17:01:28.115664   58530 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0917 17:01:28.120675   58530 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0917 17:01:28.125597   58530 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0917 17:01:28.125668   58530 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0917 17:01:28.130575   58530 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0917 17:01:28.135778   58530 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0917 17:01:28.135839   58530 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0917 17:01:28.140758   58530 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0917 17:01:28.145545   58530 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0917 17:01:28.145606   58530 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0917 17:01:28.149761   58530 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0917 17:01:28.177270   58530 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I0917 17:01:28.177293   58530 kubeadm.go:310] [preflight] Running pre-flight checks
I0917 17:01:28.217006   58530 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0917 17:01:28.217047   58530 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0917 17:01:28.217091   58530 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0917 17:01:28.222936   58530 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0917 17:01:28.231146   58530 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0917 17:01:28.231195   58530 kubeadm.go:310] [certs] Using existing ca certificate authority
I0917 17:01:28.231226   58530 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0917 17:01:28.360414   58530 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0917 17:01:28.407125   58530 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0917 17:01:28.493245   58530 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0917 17:01:28.667587   58530 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0917 17:01:28.749442   58530 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0917 17:01:28.749511   58530 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0917 17:01:28.843834   58530 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0917 17:01:28.843887   58530 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0917 17:01:28.990032   58530 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0917 17:01:29.100213   58530 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0917 17:01:29.384157   58530 kubeadm.go:310] [certs] Generating "sa" key and public key
I0917 17:01:29.384240   58530 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0917 17:01:29.490453   58530 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0917 17:01:29.666238   58530 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0917 17:01:29.785263   58530 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0917 17:01:29.846023   58530 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0917 17:01:29.960053   58530 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0917 17:01:29.960466   58530 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0917 17:01:29.962504   58530 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0917 17:01:29.967813   58530 out.go:235]     ‚ñ™ Booting up control plane ...
I0917 17:01:29.967889   58530 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0917 17:01:29.968004   58530 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0917 17:01:29.968110   58530 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0917 17:01:29.970167   58530 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0917 17:01:29.976614   58530 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0917 17:01:29.976654   58530 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0917 17:01:30.041784   58530 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0917 17:01:30.041836   58530 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0917 17:01:30.545712   58530 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 504.178375ms
I0917 17:01:30.545798   58530 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0917 17:01:34.047325   58530 kubeadm.go:310] [api-check] The API server is healthy after 3.501571751s
I0917 17:01:34.060857   58530 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0917 17:01:34.069850   58530 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0917 17:01:34.079362   58530 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0917 17:01:34.079538   58530 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0917 17:01:34.084062   58530 kubeadm.go:310] [bootstrap-token] Using token: awulbf.7v9ydtna1sjsk9pe
I0917 17:01:34.097361   58530 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0917 17:01:34.097441   58530 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0917 17:01:34.098434   58530 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0917 17:01:34.101568   58530 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0917 17:01:34.102805   58530 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0917 17:01:34.104168   58530 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0917 17:01:34.105437   58530 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0917 17:01:34.453590   58530 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0917 17:01:34.865724   58530 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0917 17:01:35.453808   58530 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0917 17:01:35.454725   58530 kubeadm.go:310] 
I0917 17:01:35.454755   58530 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0917 17:01:35.454757   58530 kubeadm.go:310] 
I0917 17:01:35.454804   58530 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0917 17:01:35.454805   58530 kubeadm.go:310] 
I0917 17:01:35.454816   58530 kubeadm.go:310]   mkdir -p $HOME/.kube
I0917 17:01:35.454843   58530 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0917 17:01:35.454862   58530 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0917 17:01:35.454864   58530 kubeadm.go:310] 
I0917 17:01:35.454900   58530 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0917 17:01:35.454902   58530 kubeadm.go:310] 
I0917 17:01:35.454938   58530 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0917 17:01:35.454942   58530 kubeadm.go:310] 
I0917 17:01:35.454964   58530 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0917 17:01:35.455021   58530 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0917 17:01:35.455063   58530 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0917 17:01:35.455066   58530 kubeadm.go:310] 
I0917 17:01:35.455111   58530 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0917 17:01:35.455154   58530 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0917 17:01:35.455156   58530 kubeadm.go:310] 
I0917 17:01:35.455210   58530 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token awulbf.7v9ydtna1sjsk9pe \
I0917 17:01:35.455269   58530 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:3cd893331475dab1fd87c942558a6f0b5333d86d25013db800c249b3a0d0b8ab \
I0917 17:01:35.455279   58530 kubeadm.go:310] 	--control-plane 
I0917 17:01:35.455280   58530 kubeadm.go:310] 
I0917 17:01:35.455323   58530 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0917 17:01:35.455337   58530 kubeadm.go:310] 
I0917 17:01:35.455378   58530 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token awulbf.7v9ydtna1sjsk9pe \
I0917 17:01:35.455430   58530 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:3cd893331475dab1fd87c942558a6f0b5333d86d25013db800c249b3a0d0b8ab 
I0917 17:01:35.458024   58530 kubeadm.go:310] W0917 10:01:28.175407    1799 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0917 17:01:35.458188   58530 kubeadm.go:310] W0917 10:01:28.175750    1799 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0917 17:01:35.458308   58530 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0917 17:01:35.458366   58530 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0917 17:01:35.458372   58530 cni.go:84] Creating CNI manager for ""
I0917 17:01:35.458384   58530 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0917 17:01:35.466342   58530 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0917 17:01:35.470489   58530 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0917 17:01:35.482194   58530 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0917 17:01:35.495358   58530 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0917 17:01:35.495445   58530 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0917 17:01:35.495458   58530 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_09_17T17_01_35_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0917 17:01:35.555766   58530 kubeadm.go:1113] duration metric: took 60.390792ms to wait for elevateKubeSystemPrivileges
I0917 17:01:35.555791   58530 ops.go:34] apiserver oom_adj: -16
I0917 17:01:35.555810   58530 kubeadm.go:394] duration metric: took 7.474466375s to StartCluster
I0917 17:01:35.555824   58530 settings.go:142] acquiring lock: {Name:mk4f7fdc11bd2d33d42e8a78f36a795d3018522c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:35.556946   58530 settings.go:150] Updating kubeconfig:  /Users/a677235/.kube/config
I0917 17:01:35.558854   58530 lock.go:35] WriteFile acquiring /Users/a677235/.kube/config: {Name:mkac8864db6bb5d9b150bd8d5b4c47b42151ace8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0917 17:01:35.560584   58530 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0917 17:01:35.560614   58530 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0917 17:01:35.560626   58530 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0917 17:01:35.560687   58530 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0917 17:01:35.560727   58530 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0917 17:01:35.560722   58530 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0917 17:01:35.560751   58530 host.go:66] Checking if "minikube" exists ...
I0917 17:01:35.560756   58530 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0917 17:01:35.561357   58530 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0917 17:01:35.562021   58530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 17:01:35.562401   58530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 17:01:35.564433   58530 out.go:177] üîé  Verifying Kubernetes components...
I0917 17:01:35.572567   58530 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0917 17:01:35.606228   58530 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0917 17:01:35.650457   58530 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0917 17:01:35.837610   58530 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0917 17:01:35.837740   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0917 17:01:35.866715   58530 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0917 17:01:35.866739   58530 host.go:66] Checking if "minikube" exists ...
I0917 17:01:35.867373   58530 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0917 17:01:35.868484   58530 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0917 17:01:35.872591   58530 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0917 17:01:35.872598   58530 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0917 17:01:35.872671   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:35.973280   58530 api_server.go:52] waiting for apiserver process to appear ...
I0917 17:01:35.973381   58530 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0917 17:01:35.979217   58530 api_server.go:72] duration metric: took 418.58425ms to wait for apiserver process to appear ...
I0917 17:01:35.979230   58530 api_server.go:88] waiting for apiserver healthz status ...
I0917 17:01:35.979244   58530 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58052/healthz ...
I0917 17:01:35.983300   58530 api_server.go:279] https://127.0.0.1:58052/healthz returned 200:
ok
I0917 17:01:35.984285   58530 api_server.go:141] control plane version: v1.31.0
I0917 17:01:35.984304   58530 api_server.go:131] duration metric: took 5.06525ms to wait for apiserver health ...
I0917 17:01:35.984312   58530 system_pods.go:43] waiting for kube-system pods to appear ...
I0917 17:01:35.989834   58530 system_pods.go:59] 4 kube-system pods found
I0917 17:01:35.989856   58530 system_pods.go:61] "etcd-minikube" [bffb0ab3-927c-42dd-9264-606669429166] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0917 17:01:35.989864   58530 system_pods.go:61] "kube-apiserver-minikube" [603bf19a-79b9-43c9-93c7-cf61e6b02376] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0917 17:01:35.989872   58530 system_pods.go:61] "kube-controller-manager-minikube" [9e74559f-d69c-46d2-a00c-08781eec55ae] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0917 17:01:35.989876   58530 system_pods.go:61] "kube-scheduler-minikube" [3f877821-518d-40c6-b97f-352801b43909] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0917 17:01:35.989880   58530 system_pods.go:74] duration metric: took 5.565083ms to wait for pod list to return data ...
I0917 17:01:35.989887   58530 kubeadm.go:582] duration metric: took 429.259ms to wait for: map[apiserver:true system_pods:true]
I0917 17:01:35.989895   58530 node_conditions.go:102] verifying NodePressure condition ...
I0917 17:01:35.992334   58530 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0917 17:01:35.992348   58530 node_conditions.go:123] node cpu capacity is 12
I0917 17:01:35.992356   58530 node_conditions.go:105] duration metric: took 2.458542ms to run NodePressure ...
I0917 17:01:35.992364   58530 start.go:241] waiting for startup goroutines ...
I0917 17:01:36.190077   58530 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0917 17:01:36.190091   58530 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0917 17:01:36.190194   58530 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0917 17:01:36.192890   58530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58048 SSHKeyPath:/Users/a677235/.minikube/machines/minikube/id_rsa Username:docker}
I0917 17:01:36.281610   58530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0917 17:01:36.325629   58530 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58048 SSHKeyPath:/Users/a677235/.minikube/machines/minikube/id_rsa Username:docker}
I0917 17:01:36.344944   58530 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0917 17:01:36.410913   58530 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0917 17:01:36.477838   58530 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0917 17:01:36.484933   58530 addons.go:510] duration metric: took 924.317125ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0917 17:01:36.484965   58530 start.go:246] waiting for cluster config update ...
I0917 17:01:36.484973   58530 start.go:255] writing updated cluster config ...
I0917 17:01:36.485958   58530 ssh_runner.go:195] Run: rm -f paused
I0917 17:01:36.953457   58530 start.go:600] kubectl: 1.28.12-dispatcher, cluster: 1.31.0 (minor skew: 3)
I0917 17:01:36.957398   58530 out.go:201] 
W0917 17:01:36.963597   58530 out.go:270] ‚ùó  /Users/a677235/Downloads/google-cloud-sdk/bin/kubectl is version 1.28.12-dispatcher, which may have incompatibilities with Kubernetes 1.31.0.
I0917 17:01:36.967396   58530 out.go:177]     ‚ñ™ Want kubectl v1.31.0? Try 'minikube kubectl -- get pods -A'
I0917 17:01:36.975451   58530 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 17 10:01:26 minikube dockerd[647]: time="2024-09-17T10:01:26.710724252Z" level=info msg="Daemon shutdown complete"
Sep 17 10:01:26 minikube systemd[1]: docker.service: Deactivated successfully.
Sep 17 10:01:26 minikube systemd[1]: Stopped Docker Application Container Engine.
Sep 17 10:01:26 minikube systemd[1]: Starting Docker Application Container Engine...
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.784609669Z" level=info msg="Starting up"
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.801116294Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.802873919Z" level=info msg="Loading containers: start."
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.847898877Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.865997336Z" level=info msg="Loading containers: done."
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.871499086Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.871538544Z" level=info msg="Daemon has completed initialization"
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.883882461Z" level=info msg="API listen on [::]:2376"
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.883889461Z" level=info msg="API listen on /var/run/docker.sock"
Sep 17 10:01:26 minikube systemd[1]: Started Docker Application Container Engine.
Sep 17 10:01:26 minikube systemd[1]: Stopping Docker Application Container Engine...
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.912540419Z" level=info msg="Processing signal 'terminated'"
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.913118752Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Sep 17 10:01:26 minikube dockerd[957]: time="2024-09-17T10:01:26.913339419Z" level=info msg="Daemon shutdown complete"
Sep 17 10:01:26 minikube systemd[1]: docker.service: Deactivated successfully.
Sep 17 10:01:26 minikube systemd[1]: Stopped Docker Application Container Engine.
Sep 17 10:01:26 minikube systemd[1]: Starting Docker Application Container Engine...
Sep 17 10:01:26 minikube dockerd[1224]: time="2024-09-17T10:01:26.939951544Z" level=info msg="Starting up"
Sep 17 10:01:26 minikube dockerd[1224]: time="2024-09-17T10:01:26.948390919Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Sep 17 10:01:26 minikube dockerd[1224]: time="2024-09-17T10:01:26.953352294Z" level=info msg="Loading containers: start."
Sep 17 10:01:26 minikube dockerd[1224]: time="2024-09-17T10:01:26.989412336Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 17 10:01:27 minikube dockerd[1224]: time="2024-09-17T10:01:27.002960044Z" level=info msg="Loading containers: done."
Sep 17 10:01:27 minikube dockerd[1224]: time="2024-09-17T10:01:27.008279294Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Sep 17 10:01:27 minikube dockerd[1224]: time="2024-09-17T10:01:27.008328544Z" level=info msg="Daemon has completed initialization"
Sep 17 10:01:27 minikube dockerd[1224]: time="2024-09-17T10:01:27.019707836Z" level=info msg="API listen on /var/run/docker.sock"
Sep 17 10:01:27 minikube dockerd[1224]: time="2024-09-17T10:01:27.019711502Z" level=info msg="API listen on [::]:2376"
Sep 17 10:01:27 minikube systemd[1]: Started Docker Application Container Engine.
Sep 17 10:01:27 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Start docker client with request timeout 0s"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Hairpin mode is set to hairpin-veth"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Loaded network plugin cni"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Docker cri networking managed by network plugin cni"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Setting cgroupDriver cgroupfs"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Sep 17 10:01:27 minikube cri-dockerd[1496]: time="2024-09-17T10:01:27Z" level=info msg="Start cri-dockerd grpc backend"
Sep 17 10:01:27 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Sep 17 10:01:30 minikube cri-dockerd[1496]: time="2024-09-17T10:01:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b5eabba13c4ba65f3ec19aeb4c7ff8d4f95184dd1714842f16bb6e1280d05b41/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 10:01:30 minikube cri-dockerd[1496]: time="2024-09-17T10:01:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/099ebf203e494cc7c15c9468244b62b85f9ae78b887df56a9886bf6e2de14ad9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 10:01:30 minikube cri-dockerd[1496]: time="2024-09-17T10:01:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f82d76e88a91a6b7e74e726d34dc859e539a6b9b9c772b60da82e87706b7afdc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 10:01:30 minikube cri-dockerd[1496]: time="2024-09-17T10:01:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2122e78e11a80197e2091b84ddb530ec64d24b555b2e69a5ccf013be47e232f8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 10:01:40 minikube cri-dockerd[1496]: time="2024-09-17T10:01:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/520f1e60b420b876f415e396764defbec1303e15062b640e8cf84f5a67c62b99/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 10:01:40 minikube cri-dockerd[1496]: time="2024-09-17T10:01:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea05b9b2847f2c46f71ebc6154be97b988f00e8000e96d10bfe71dcbbef20f4a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 10:01:40 minikube cri-dockerd[1496]: time="2024-09-17T10:01:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/301dac171fa3eb2c6972c8227f4dcbc1e877301748f054f5d479d4579e5cf2c8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 17 10:01:44 minikube cri-dockerd[1496]: time="2024-09-17T10:01:44Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Sep 17 10:02:10 minikube dockerd[1224]: time="2024-09-17T10:02:10.545302426Z" level=info msg="ignoring event" container=1ff487b2c7da416afe7049f38641c2896426e98d4e50f5b584b926e5798d6a1d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 17 10:17:37 minikube cri-dockerd[1496]: time="2024-09-17T10:17:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3452a89c14d768007886eb71f91af1e01ac738e9e161d0599d50ac8341445dcd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 10:17:48 minikube cri-dockerd[1496]: time="2024-09-17T10:17:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0927b146cb7813d3556eee4aa4845e61b7ae84505fce46ec0759afd355bc317c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 17 10:17:51 minikube dockerd[1224]: time="2024-09-17T10:17:51.851916125Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 10:17:51 minikube dockerd[1224]: time="2024-09-17T10:17:51.851993083Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 10:18:09 minikube dockerd[1224]: time="2024-09-17T10:18:09.750143800Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 10:18:09 minikube dockerd[1224]: time="2024-09-17T10:18:09.750250508Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 17 10:18:37 minikube dockerd[1224]: time="2024-09-17T10:18:37.744243591Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 17 10:18:37 minikube dockerd[1224]: time="2024-09-17T10:18:37.744336091Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
8baa172b8ceef       a4b4f96ab402e       About a minute ago   Running             service-a                 0                   3452a89c14d76       service-a-76bb7cff64-xzkkm
c4cbfe37a32e2       ba04bb24b9575       17 minutes ago       Running             storage-provisioner       1                   ea05b9b2847f2       storage-provisioner
7471e6ce838e4       2437cf7621777       17 minutes ago       Running             coredns                   0                   301dac171fa3e       coredns-6f6b679f8f-rngsz
1ff487b2c7da4       ba04bb24b9575       17 minutes ago       Exited              storage-provisioner       0                   ea05b9b2847f2       storage-provisioner
ac9799e894d6a       71d55d66fd4ee       17 minutes ago       Running             kube-proxy                0                   520f1e60b420b       kube-proxy-562cm
b2130e41707ff       fcb0683e6bdbd       17 minutes ago       Running             kube-controller-manager   0                   099ebf203e494       kube-controller-manager-minikube
b0fa45ed5842d       fbbbd428abb4d       17 minutes ago       Running             kube-scheduler            0                   f82d76e88a91a       kube-scheduler-minikube
053eb1c9f0865       cd0f0ae0ec9e0       17 minutes ago       Running             kube-apiserver            0                   b5eabba13c4ba       kube-apiserver-minikube
f6fe5a3b108ce       27e3830e14027       17 minutes ago       Running             etcd                      0                   2122e78e11a80       etcd-minikube


==> coredns [7471e6ce838e] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/arm64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:46439 - 49034 "HINFO IN 5666821136261844865.8159867604400485153. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.08587525s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_17T17_01_35_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 17 Sep 2024 10:01:32 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 17 Sep 2024 10:19:16 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 17 Sep 2024 10:18:13 +0000   Tue, 17 Sep 2024 10:01:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 17 Sep 2024 10:18:13 +0000   Tue, 17 Sep 2024 10:01:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 17 Sep 2024 10:18:13 +0000   Tue, 17 Sep 2024 10:01:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 17 Sep 2024 10:18:13 +0000   Tue, 17 Sep 2024 10:01:32 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8028732Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8028732Ki
  pods:               110
System Info:
  Machine ID:                 f46397ec038f49668cb67143eb7a85c6
  System UUID:                f46397ec038f49668cb67143eb7a85c6
  Boot ID:                    5ea850a2-2980-4510-b139-b8e186620cda
  Kernel Version:             6.6.31-linuxkit
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     service-a-76bb7cff64-xzkkm          0 (0%)        0 (0%)      0 (0%)           0 (0%)         106s
  default                     service-b-8587c97ccf-2qfqk          0 (0%)        0 (0%)      0 (0%)           0 (0%)         94s
  kube-system                 coredns-6f6b679f8f-rngsz            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     17m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         17m
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-proxy-562cm                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 17m                kube-proxy       
  Normal  Starting                 17m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m (x7 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 17m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  17m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  17m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    17m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     17m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           17m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Sep16 12:50] systemd[18598]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[Sep17 03:00] cfs_period_timer[cpu7]: period too short, scaling up (new cfs_period_us = 200000, cfs_quota_us = 400000)


==> etcd [f6fe5a3b108c] <==
{"level":"warn","ts":"2024-09-17T10:01:30.970470Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-09-17T10:01:30.970575Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-09-17T10:01:30.970641Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-09-17T10:01:30.970695Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-09-17T10:01:30.970869Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-09-17T10:01:30.971437Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-09-17T10:01:30.971515Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"arm64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-09-17T10:01:30.972812Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.086625ms"}
{"level":"info","ts":"2024-09-17T10:01:30.974936Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-09-17T10:01:30.974981Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-09-17T10:01:30.975052Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-09-17T10:01:30.975060Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-09-17T10:01:30.975063Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-09-17T10:01:30.975081Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-09-17T10:01:30.978887Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-09-17T10:01:30.979416Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-09-17T10:01:30.979681Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-09-17T10:01:30.980675Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-09-17T10:01:30.980932Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-17T10:01:31.032941Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-09-17T10:01:31.033433Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T10:01:31.033476Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T10:01:31.033481Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T10:01:31.034602Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-09-17T10:01:31.034678Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-09-17T10:01:31.034988Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-09-17T10:01:31.035342Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-09-17T10:01:31.035374Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-09-17T10:01:31.035364Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T10:01:31.035438Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T10:01:31.176548Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-09-17T10:01:31.176584Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-09-17T10:01:31.176636Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-09-17T10:01:31.176659Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-09-17T10:01:31.176665Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-09-17T10:01:31.176689Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-09-17T10:01:31.176696Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-09-17T10:01:31.177509Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T10:01:31.177573Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-09-17T10:01:31.177731Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-17T10:01:31.177839Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T10:01:31.177873Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T10:01:31.177888Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T10:01:31.177949Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-09-17T10:01:31.177960Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-09-17T10:01:31.177963Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-17T10:01:31.178610Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-17T10:01:31.178632Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-09-17T10:01:31.179310Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-09-17T10:01:31.179645Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-09-17T10:11:18.318090Z","caller":"traceutil/trace.go:171","msg":"trace[569062908] transaction","detail":"{read_only:false; response_revision:863; number_of_response:1; }","duration":"211.126376ms","start":"2024-09-17T10:11:18.106931Z","end":"2024-09-17T10:11:18.318057Z","steps":["trace[569062908] 'process raft request'  (duration: 211.015876ms)"],"step_count":1}
{"level":"info","ts":"2024-09-17T10:11:31.384398Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":634}
{"level":"info","ts":"2024-09-17T10:11:31.388335Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":634,"took":"3.590458ms","hash":1111512963,"current-db-size-bytes":1380352,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1380352,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-17T10:11:31.388387Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1111512963,"revision":634,"compact-revision":-1}
{"level":"info","ts":"2024-09-17T10:16:31.386182Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":873}
{"level":"info","ts":"2024-09-17T10:16:31.388784Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":873,"took":"2.142542ms","hash":2186106280,"current-db-size-bytes":1380352,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":917504,"current-db-size-in-use":"918 kB"}
{"level":"info","ts":"2024-09-17T10:16:31.388831Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2186106280,"revision":873,"compact-revision":634}
{"level":"info","ts":"2024-09-17T10:17:34.475200Z","caller":"traceutil/trace.go:171","msg":"trace[1800164245] transaction","detail":"{read_only:false; response_revision:1165; number_of_response:1; }","duration":"170.80625ms","start":"2024-09-17T10:17:34.304354Z","end":"2024-09-17T10:17:34.475160Z","steps":["trace[1800164245] 'process raft request'  (duration: 128.158583ms)","trace[1800164245] 'compare'  (duration: 42.547875ms)"],"step_count":2}


==> kernel <==
 10:19:22 up 9 days, 15:01,  0 users,  load average: 3.35, 3.06, 2.96
Linux minikube 6.6.31-linuxkit #1 SMP Thu May 23 08:36:57 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [053eb1c9f086] <==
I0917 10:01:32.367452       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0917 10:01:32.367462       1 local_available_controller.go:156] Starting LocalAvailability controller
I0917 10:01:32.367464       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0917 10:01:32.367498       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0917 10:01:32.367516       1 controller.go:119] Starting legacy_token_tracking_controller
I0917 10:01:32.367520       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0917 10:01:32.367617       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0917 10:01:32.367631       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0917 10:01:32.367643       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0917 10:01:32.367677       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0917 10:01:32.367637       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0917 10:01:32.367770       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0917 10:01:32.367776       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0917 10:01:32.367780       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0917 10:01:32.367655       1 controller.go:78] Starting OpenAPI AggregationController
I0917 10:01:32.367733       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0917 10:01:32.367873       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0917 10:01:32.367754       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0917 10:01:32.367983       1 controller.go:142] Starting OpenAPI controller
I0917 10:01:32.368120       1 controller.go:90] Starting OpenAPI V3 controller
I0917 10:01:32.368164       1 naming_controller.go:294] Starting NamingConditionController
I0917 10:01:32.368234       1 establishing_controller.go:81] Starting EstablishingController
I0917 10:01:32.368263       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0917 10:01:32.368278       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0917 10:01:32.368287       1 crd_finalizer.go:269] Starting CRDFinalizer
I0917 10:01:32.378676       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0917 10:01:32.378706       1 policy_source.go:224] refreshing policies
I0917 10:01:32.469537       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0917 10:01:32.469539       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0917 10:01:32.469580       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0917 10:01:32.469583       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0917 10:01:32.469602       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0917 10:01:32.469616       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0917 10:01:32.469631       1 aggregator.go:171] initial CRD sync complete...
I0917 10:01:32.469657       1 autoregister_controller.go:144] Starting autoregister controller
I0917 10:01:32.469662       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0917 10:01:32.469666       1 cache.go:39] Caches are synced for autoregister controller
I0917 10:01:32.469684       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0917 10:01:32.469717       1 cache.go:39] Caches are synced for LocalAvailability controller
I0917 10:01:32.469735       1 shared_informer.go:320] Caches are synced for configmaps
I0917 10:01:32.470421       1 controller.go:615] quota admission added evaluator for: namespaces
I0917 10:01:32.473631       1 shared_informer.go:320] Caches are synced for node_authorizer
I0917 10:01:32.533730       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0917 10:01:33.373894       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0917 10:01:33.376734       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0917 10:01:33.376759       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0917 10:01:33.629367       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0917 10:01:33.641375       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0917 10:01:33.673357       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0917 10:01:33.675764       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0917 10:01:33.676268       1 controller.go:615] quota admission added evaluator for: endpoints
I0917 10:01:33.678091       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0917 10:01:34.454658       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0917 10:01:34.856707       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0917 10:01:34.863842       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0917 10:01:34.869983       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0917 10:01:40.007235       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0917 10:01:40.107462       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0917 10:17:36.937822       1 alloc.go:330] "allocated clusterIPs" service="default/service-a" clusterIPs={"IPv4":"10.100.126.106"}
I0917 10:17:48.450398       1 alloc.go:330] "allocated clusterIPs" service="default/service-b" clusterIPs={"IPv4":"10.110.253.228"}


==> kube-controller-manager [b2130e41707f] <==
I0917 10:01:39.277941       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:01:39.278271       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:01:39.303658       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0917 10:01:39.362431       1 shared_informer.go:320] Caches are synced for resource quota
I0917 10:01:39.386984       1 shared_informer.go:320] Caches are synced for daemon sets
I0917 10:01:39.399595       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0917 10:01:39.402298       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0917 10:01:39.402342       1 shared_informer.go:320] Caches are synced for disruption
I0917 10:01:39.404628       1 shared_informer.go:320] Caches are synced for job
I0917 10:01:39.404669       1 shared_informer.go:320] Caches are synced for taint
I0917 10:01:39.404676       1 shared_informer.go:320] Caches are synced for PVC protection
I0917 10:01:39.404780       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0917 10:01:39.404845       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0917 10:01:39.404905       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0917 10:01:39.404956       1 shared_informer.go:320] Caches are synced for persistent volume
I0917 10:01:39.405010       1 shared_informer.go:320] Caches are synced for HPA
I0917 10:01:39.405156       1 shared_informer.go:320] Caches are synced for stateful set
I0917 10:01:39.405174       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0917 10:01:39.407495       1 shared_informer.go:320] Caches are synced for deployment
I0917 10:01:39.407801       1 shared_informer.go:320] Caches are synced for resource quota
I0917 10:01:39.415750       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:01:39.443678       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0917 10:01:39.452019       1 shared_informer.go:320] Caches are synced for attach detach
I0917 10:01:39.454729       1 shared_informer.go:320] Caches are synced for endpoint
I0917 10:01:39.454738       1 shared_informer.go:320] Caches are synced for ephemeral
I0917 10:01:39.456445       1 shared_informer.go:320] Caches are synced for GC
I0917 10:01:39.458428       1 shared_informer.go:320] Caches are synced for ReplicationController
I0917 10:01:39.824665       1 shared_informer.go:320] Caches are synced for garbage collector
I0917 10:01:39.854499       1 shared_informer.go:320] Caches are synced for garbage collector
I0917 10:01:39.854529       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0917 10:01:39.959942       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:01:40.316056       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="207.174875ms"
I0917 10:01:40.320704       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="4.602042ms"
I0917 10:01:40.320858       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="114.042¬µs"
I0917 10:01:40.320958       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="30.625¬µs"
I0917 10:01:40.322872       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="36.833¬µs"
I0917 10:01:41.659940       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="63.5¬µs"
I0917 10:01:42.782244       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="7.697792ms"
I0917 10:01:42.782316       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="36.291¬µs"
I0917 10:01:44.732604       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:06:49.712015       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:11:55.816455       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:13:07.434314       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:17:36.539638       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-a-76bb7cff64" duration="14.742541ms"
I0917 10:17:36.547328       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-a-76bb7cff64" duration="7.533375ms"
I0917 10:17:36.547412       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-a-76bb7cff64" duration="43.834¬µs"
I0917 10:17:36.556542       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-a-76bb7cff64" duration="29.875¬µs"
I0917 10:17:37.687634       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-a-76bb7cff64" duration="3.89025ms"
I0917 10:17:37.687677       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-a-76bb7cff64" duration="18.125¬µs"
I0917 10:17:48.093470       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="8.930583ms"
I0917 10:17:48.098266       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="4.7255ms"
I0917 10:17:48.098350       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="33.5¬µs"
I0917 10:17:48.106216       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="98.125¬µs"
I0917 10:17:52.801623       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="116.917¬µs"
I0917 10:18:06.585262       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="178¬µs"
I0917 10:18:13.202781       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0917 10:18:23.584186       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="132.583¬µs"
I0917 10:18:34.584245       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="397.625¬µs"
I0917 10:18:50.581848       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="95.792¬µs"
I0917 10:19:01.581283       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/service-b-8587c97ccf" duration="72.875¬µs"


==> kube-proxy [ac9799e894d6] <==
I0917 10:01:40.582169       1 server_linux.go:66] "Using iptables proxy"
I0917 10:01:40.641014       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0917 10:01:40.641074       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0917 10:01:40.649981       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0917 10:01:40.650022       1 server_linux.go:169] "Using iptables Proxier"
I0917 10:01:40.650749       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0917 10:01:40.651267       1 server.go:483] "Version info" version="v1.31.0"
I0917 10:01:40.651305       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 10:01:40.653820       1 config.go:197] "Starting service config controller"
I0917 10:01:40.653871       1 config.go:104] "Starting endpoint slice config controller"
I0917 10:01:40.653925       1 config.go:326] "Starting node config controller"
I0917 10:01:40.653942       1 shared_informer.go:313] Waiting for caches to sync for node config
I0917 10:01:40.653941       1 shared_informer.go:313] Waiting for caches to sync for service config
I0917 10:01:40.653945       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0917 10:01:40.754433       1 shared_informer.go:320] Caches are synced for node config
I0917 10:01:40.754458       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0917 10:01:40.754477       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [b0fa45ed5842] <==
I0917 10:01:31.881917       1 serving.go:386] Generated self-signed cert in-memory
W0917 10:01:32.436970       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0917 10:01:32.437009       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0917 10:01:32.437021       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0917 10:01:32.437025       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0917 10:01:32.445880       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0917 10:01:32.445898       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 10:01:32.446998       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0917 10:01:32.447083       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 10:01:32.447110       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0917 10:01:32.447142       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0917 10:01:32.448164       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0917 10:01:32.448198       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0917 10:01:32.448199       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
E0917 10:01:32.448211       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448338       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0917 10:01:32.448356       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448359       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0917 10:01:32.448374       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448384       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0917 10:01:32.448393       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448414       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0917 10:01:32.448427       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448443       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0917 10:01:32.448450       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448462       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0917 10:01:32.448491       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0917 10:01:32.448497       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0917 10:01:32.448498       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448491       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0917 10:01:32.448516       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0917 10:01:32.448518       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0917 10:01:32.448526       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448535       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0917 10:01:32.448545       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448535       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0917 10:01:32.448558       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0917 10:01:32.448562       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0917 10:01:32.448569       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 10:01:32.448622       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0917 10:01:32.448638       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 10:01:33.303413       1 reflector.go:561] runtime/asm_arm64.s:1222: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0917 10:01:33.303456       1 reflector.go:158] "Unhandled Error" err="runtime/asm_arm64.s:1222: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0917 10:01:33.324983       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0917 10:01:33.325027       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0917 10:01:33.335643       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0917 10:01:33.335685       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0917 10:01:33.421834       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0917 10:01:33.421867       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0917 10:01:33.464875       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0917 10:01:33.464903       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
I0917 10:01:36.447298       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.875534    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/a5363f4f31e043bdae3c93aca4991903-etcd-certs\") pod \"etcd-minikube\" (UID: \"a5363f4f31e043bdae3c93aca4991903\") " pod="kube-system/etcd-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876555    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/a5363f4f31e043bdae3c93aca4991903-etcd-data\") pod \"etcd-minikube\" (UID: \"a5363f4f31e043bdae3c93aca4991903\") " pod="kube-system/etcd-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876590    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876625    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876645    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876675    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876696    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876714    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.876733    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.877261    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.877296    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.877316    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.877333    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Sep 17 10:01:34 minikube kubelet[2310]: I0917 10:01:34.877351    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/9e315b3a91fa9f6f7463439d9dac1a56-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"9e315b3a91fa9f6f7463439d9dac1a56\") " pod="kube-system/kube-apiserver-minikube"
Sep 17 10:01:35 minikube kubelet[2310]: I0917 10:01:35.569304    2310 apiserver.go:52] "Watching apiserver"
Sep 17 10:01:35 minikube kubelet[2310]: I0917 10:01:35.573322    2310 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Sep 17 10:01:35 minikube kubelet[2310]: E0917 10:01:35.602824    2310 kubelet.go:1915] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Sep 17 10:01:35 minikube kubelet[2310]: E0917 10:01:35.603395    2310 kubelet.go:1915] "Failed creating a mirror pod for" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Sep 17 10:01:35 minikube kubelet[2310]: I0917 10:01:35.654641    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.654629132 podStartE2EDuration="1.654629132s" podCreationTimestamp="2024-09-17 10:01:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:01:35.65447284 +0000 UTC m=+1.115589335" watchObservedRunningTime="2024-09-17 10:01:35.654629132 +0000 UTC m=+1.115745544"
Sep 17 10:01:35 minikube kubelet[2310]: I0917 10:01:35.739488    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.739469715 podStartE2EDuration="1.739469715s" podCreationTimestamp="2024-09-17 10:01:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:01:35.663577382 +0000 UTC m=+1.124693794" watchObservedRunningTime="2024-09-17 10:01:35.739469715 +0000 UTC m=+1.200586210"
Sep 17 10:01:35 minikube kubelet[2310]: I0917 10:01:35.748634    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.748612923 podStartE2EDuration="1.748612923s" podCreationTimestamp="2024-09-17 10:01:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:01:35.73967759 +0000 UTC m=+1.200794127" watchObservedRunningTime="2024-09-17 10:01:35.748612923 +0000 UTC m=+1.209729419"
Sep 17 10:01:35 minikube kubelet[2310]: I0917 10:01:35.748829    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.748819132 podStartE2EDuration="1.748819132s" podCreationTimestamp="2024-09-17 10:01:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:01:35.748591215 +0000 UTC m=+1.209707752" watchObservedRunningTime="2024-09-17 10:01:35.748819132 +0000 UTC m=+1.209935669"
Sep 17 10:01:39 minikube kubelet[2310]: I0917 10:01:39.488034    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/3fd61d2b-3e40-406d-8cf0-01d2015cd1f0-tmp\") pod \"storage-provisioner\" (UID: \"3fd61d2b-3e40-406d-8cf0-01d2015cd1f0\") " pod="kube-system/storage-provisioner"
Sep 17 10:01:39 minikube kubelet[2310]: I0917 10:01:39.488171    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ffdtv\" (UniqueName: \"kubernetes.io/projected/3fd61d2b-3e40-406d-8cf0-01d2015cd1f0-kube-api-access-ffdtv\") pod \"storage-provisioner\" (UID: \"3fd61d2b-3e40-406d-8cf0-01d2015cd1f0\") " pod="kube-system/storage-provisioner"
Sep 17 10:01:39 minikube kubelet[2310]: E0917 10:01:39.595027    2310 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Sep 17 10:01:39 minikube kubelet[2310]: E0917 10:01:39.595052    2310 projected.go:194] Error preparing data for projected volume kube-api-access-ffdtv for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Sep 17 10:01:39 minikube kubelet[2310]: E0917 10:01:39.595106    2310 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/3fd61d2b-3e40-406d-8cf0-01d2015cd1f0-kube-api-access-ffdtv podName:3fd61d2b-3e40-406d-8cf0-01d2015cd1f0 nodeName:}" failed. No retries permitted until 2024-09-17 10:01:40.095087717 +0000 UTC m=+5.556204170 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-ffdtv" (UniqueName: "kubernetes.io/projected/3fd61d2b-3e40-406d-8cf0-01d2015cd1f0-kube-api-access-ffdtv") pod "storage-provisioner" (UID: "3fd61d2b-3e40-406d-8cf0-01d2015cd1f0") : configmap "kube-root-ca.crt" not found
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.102399    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zrzdc\" (UniqueName: \"kubernetes.io/projected/264236ad-ac87-4c53-9343-277678e9ed06-kube-api-access-zrzdc\") pod \"kube-proxy-562cm\" (UID: \"264236ad-ac87-4c53-9343-277678e9ed06\") " pod="kube-system/kube-proxy-562cm"
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.102522    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/264236ad-ac87-4c53-9343-277678e9ed06-lib-modules\") pod \"kube-proxy-562cm\" (UID: \"264236ad-ac87-4c53-9343-277678e9ed06\") " pod="kube-system/kube-proxy-562cm"
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.102603    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/264236ad-ac87-4c53-9343-277678e9ed06-kube-proxy\") pod \"kube-proxy-562cm\" (UID: \"264236ad-ac87-4c53-9343-277678e9ed06\") " pod="kube-system/kube-proxy-562cm"
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.102637    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/264236ad-ac87-4c53-9343-277678e9ed06-xtables-lock\") pod \"kube-proxy-562cm\" (UID: \"264236ad-ac87-4c53-9343-277678e9ed06\") " pod="kube-system/kube-proxy-562cm"
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.406405    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/4c2e8027-eb81-4e81-a1fd-a612163b141b-config-volume\") pod \"coredns-6f6b679f8f-rngsz\" (UID: \"4c2e8027-eb81-4e81-a1fd-a612163b141b\") " pod="kube-system/coredns-6f6b679f8f-rngsz"
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.406440    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xrmkk\" (UniqueName: \"kubernetes.io/projected/4c2e8027-eb81-4e81-a1fd-a612163b141b-kube-api-access-xrmkk\") pod \"coredns-6f6b679f8f-rngsz\" (UID: \"4c2e8027-eb81-4e81-a1fd-a612163b141b\") " pod="kube-system/coredns-6f6b679f8f-rngsz"
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.633019    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=4.633007009 podStartE2EDuration="4.633007009s" podCreationTimestamp="2024-09-17 10:01:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:01:40.632934676 +0000 UTC m=+6.094051171" watchObservedRunningTime="2024-09-17 10:01:40.633007009 +0000 UTC m=+6.094123546"
Sep 17 10:01:40 minikube kubelet[2310]: I0917 10:01:40.639524    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-562cm" podStartSLOduration=0.639511009 podStartE2EDuration="639.511009ms" podCreationTimestamp="2024-09-17 10:01:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:01:40.639414634 +0000 UTC m=+6.100531129" watchObservedRunningTime="2024-09-17 10:01:40.639511009 +0000 UTC m=+6.100627463"
Sep 17 10:01:41 minikube kubelet[2310]: I0917 10:01:41.659951    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-6f6b679f8f-rngsz" podStartSLOduration=1.6599187180000001 podStartE2EDuration="1.659918718s" podCreationTimestamp="2024-09-17 10:01:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:01:41.659786176 +0000 UTC m=+7.120902713" watchObservedRunningTime="2024-09-17 10:01:41.659918718 +0000 UTC m=+7.121035255"
Sep 17 10:01:42 minikube kubelet[2310]: I0917 10:01:42.765025    2310 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Sep 17 10:01:44 minikube kubelet[2310]: I0917 10:01:44.725100    2310 kuberuntime_manager.go:1633] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Sep 17 10:01:44 minikube kubelet[2310]: I0917 10:01:44.726010    2310 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Sep 17 10:02:10 minikube kubelet[2310]: I0917 10:02:10.910332    2310 scope.go:117] "RemoveContainer" containerID="1ff487b2c7da416afe7049f38641c2896426e98d4e50f5b584b926e5798d6a1d"
Sep 17 10:17:36 minikube kubelet[2310]: I0917 10:17:36.657157    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wkjnk\" (UniqueName: \"kubernetes.io/projected/0108f890-c99b-430d-94e7-bbda27a39bee-kube-api-access-wkjnk\") pod \"service-a-76bb7cff64-xzkkm\" (UID: \"0108f890-c99b-430d-94e7-bbda27a39bee\") " pod="default/service-a-76bb7cff64-xzkkm"
Sep 17 10:17:37 minikube kubelet[2310]: I0917 10:17:37.683705    2310 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/service-a-76bb7cff64-xzkkm" podStartSLOduration=1.6836619659999998 podStartE2EDuration="1.683661966s" podCreationTimestamp="2024-09-17 10:17:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-17 10:17:37.683606632 +0000 UTC m=+963.154583439" watchObservedRunningTime="2024-09-17 10:17:37.683661966 +0000 UTC m=+963.154638689"
Sep 17 10:17:48 minikube kubelet[2310]: I0917 10:17:48.260563    2310 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gw8c4\" (UniqueName: \"kubernetes.io/projected/5fd41066-6032-45bb-9849-ac7ae0d7e72d-kube-api-access-gw8c4\") pod \"service-b-8587c97ccf-2qfqk\" (UID: \"5fd41066-6032-45bb-9849-ac7ae0d7e72d\") " pod="default/service-b-8587c97ccf-2qfqk"
Sep 17 10:17:51 minikube kubelet[2310]: E0917 10:17:51.854476    2310 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="service-b:latest"
Sep 17 10:17:51 minikube kubelet[2310]: E0917 10:17:51.854588    2310 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="service-b:latest"
Sep 17 10:17:51 minikube kubelet[2310]: E0917 10:17:51.854820    2310 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:service-b,Image:service-b:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:SECRET_KEY,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:my-secret,},Key:SECRET_KEY,Optional:nil,},},},EnvVar{Name:CONFIG_MAP_KEY,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:my-config,},Key:CONFIG_MAP_KEY,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gw8c4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod service-b-8587c97ccf-2qfqk_default(5fd41066-6032-45bb-9849-ac7ae0d7e72d): ErrImagePull: Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 17 10:17:51 minikube kubelet[2310]: E0917 10:17:51.856663    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ErrImagePull: \"Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"
Sep 17 10:17:52 minikube kubelet[2310]: E0917 10:17:52.793670    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ImagePullBackOff: \"Back-off pulling image \\\"service-b:latest\\\"\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"
Sep 17 10:18:09 minikube kubelet[2310]: E0917 10:18:09.754004    2310 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="service-b:latest"
Sep 17 10:18:09 minikube kubelet[2310]: E0917 10:18:09.754079    2310 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="service-b:latest"
Sep 17 10:18:09 minikube kubelet[2310]: E0917 10:18:09.754239    2310 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:service-b,Image:service-b:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:SECRET_KEY,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:my-secret,},Key:SECRET_KEY,Optional:nil,},},},EnvVar{Name:CONFIG_MAP_KEY,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:my-config,},Key:CONFIG_MAP_KEY,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gw8c4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod service-b-8587c97ccf-2qfqk_default(5fd41066-6032-45bb-9849-ac7ae0d7e72d): ErrImagePull: Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 17 10:18:09 minikube kubelet[2310]: E0917 10:18:09.755644    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ErrImagePull: \"Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"
Sep 17 10:18:23 minikube kubelet[2310]: E0917 10:18:23.573045    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ImagePullBackOff: \"Back-off pulling image \\\"service-b:latest\\\"\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"
Sep 17 10:18:37 minikube kubelet[2310]: E0917 10:18:37.746795    2310 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="service-b:latest"
Sep 17 10:18:37 minikube kubelet[2310]: E0917 10:18:37.746877    2310 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="service-b:latest"
Sep 17 10:18:37 minikube kubelet[2310]: E0917 10:18:37.747063    2310 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:service-b,Image:service-b:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:SECRET_KEY,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:my-secret,},Key:SECRET_KEY,Optional:nil,},},},EnvVar{Name:CONFIG_MAP_KEY,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:my-config,},Key:CONFIG_MAP_KEY,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gw8c4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod service-b-8587c97ccf-2qfqk_default(5fd41066-6032-45bb-9849-ac7ae0d7e72d): ErrImagePull: Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 17 10:18:37 minikube kubelet[2310]: E0917 10:18:37.748352    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ErrImagePull: \"Error response from daemon: pull access denied for service-b, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"
Sep 17 10:18:50 minikube kubelet[2310]: E0917 10:18:50.574388    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ImagePullBackOff: \"Back-off pulling image \\\"service-b:latest\\\"\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"
Sep 17 10:19:01 minikube kubelet[2310]: E0917 10:19:01.572324    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ImagePullBackOff: \"Back-off pulling image \\\"service-b:latest\\\"\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"
Sep 17 10:19:16 minikube kubelet[2310]: E0917 10:19:16.574875    2310 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"service-b\" with ImagePullBackOff: \"Back-off pulling image \\\"service-b:latest\\\"\"" pod="default/service-b-8587c97ccf-2qfqk" podUID="5fd41066-6032-45bb-9849-ac7ae0d7e72d"


==> storage-provisioner [1ff487b2c7da] <==
I0917 10:01:40.518717       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0917 10:02:10.522992       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [c4cbfe37a32e] <==
I0917 10:02:11.074351       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0917 10:02:11.081351       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0917 10:02:11.081417       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0917 10:02:11.090045       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0917 10:02:11.090238       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a0170657-b66a-4a12-afd6-5d807bb70a60!
I0917 10:02:11.090258       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"b75ae471-02ba-4134-9977-0d5f2792f1b8", APIVersion:"v1", ResourceVersion:"424", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a0170657-b66a-4a12-afd6-5d807bb70a60 became leader
I0917 10:02:11.193227       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a0170657-b66a-4a12-afd6-5d807bb70a60!

